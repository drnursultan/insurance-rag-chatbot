from .config import LLM_PROVIDER, OPENAI_MODEL
def load_llm():
    if LLM_PROVIDER == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=OPENAI_MODEL, temperature=0)
    else:
        # small, free HF instruct model for Colab/local CPU/GPU
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        from langchain.llms import HuggingFacePipeline
        #name = "google/gemma-2-2b-it"
        name = "mistralai/Mistral-7B-Instruct-v0.2"
	tok = AutoTokenizer.from_pretrained(name)
        mdl = AutoModelForCausalLM.from_pretrained(name, device_map="auto", torch_dtype="auto")
        gen = pipeline("text-generation", model=mdl, tokenizer=tok, max_new_tokens=384, do_sample=False)
        return HuggingFacePipeline(pipeline=gen)
